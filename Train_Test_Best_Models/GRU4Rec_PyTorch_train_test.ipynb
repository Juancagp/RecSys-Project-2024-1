{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to CodeCarbon, here is your experiment id:\n",
      "8c5eece3-1309-4aea-a822-e8f636025071 (from ./.codecarbon.config)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! codecarbon init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from experiment_setup import setups\n",
    "import torch\n",
    "from codecarbon import EmissionsTracker\n",
    "from datetime import datetime\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "import subprocess\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_training_C02_emissions_carbontracker(command, trained_model_folder):\n",
    "    # Inicializamos el tracker\n",
    "    tracker = CarbonTracker(epochs=1)  # Usamos epochs=1 para rastrear toda la ejecución como una única época\n",
    "\n",
    "    try:\n",
    "        # Obtenemos la fecha y hora de inicio\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Iniciamos el tracker\n",
    "        tracker.epoch_start()\n",
    "\n",
    "        # Ejecutamos el comando de entrenamiento\n",
    "        training_process = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "        # Detenemos el tracker y obtenemos las emisiones finales\n",
    "        tracker.epoch_end()\n",
    "        tracker.stop()  # Detenemos el tracker al finalizar\n",
    "\n",
    "        # Obtenemos la fecha y hora de finalización\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        # Imprimimos la salida de la ejecución\n",
    "        print(f\"Salida de STDOUT: {training_process.stdout}\")\n",
    "\n",
    "        # Obtén las emisiones de CO2\n",
    "        emissions = tracker.final_emissions\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Ruta del archivo JSON\n",
    "    json_file_path = os.path.join(\"..\", \"trained_models\", trained_model_folder, \"trainingData.json\")\n",
    "\n",
    "    # Leer el archivo JSON existente\n",
    "    existing_data = []\n",
    "    if os.path.exists(json_file_path):\n",
    "        try:\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                existing_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"El archivo {json_file_path} está vacío o contiene datos inválidos, se inicializará como una lista vacía.\")\n",
    "            existing_data = []\n",
    "\n",
    "    # Preparar la información del entrenamiento\n",
    "    training_info = {\n",
    "        \"training_iteration\": len(existing_data) + 1,  # Número de iteración basado en el tamaño del dataset existente\n",
    "        \"date\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"execution_time_seconds\": (end_time - start_time).total_seconds(),\n",
    "        \"CO2_emissions_kg\": emissions\n",
    "    }\n",
    "\n",
    "    # Agregar la nueva información del entrenamiento\n",
    "    existing_data.append(training_info)\n",
    "\n",
    "    # Escribir los datos actualizados al archivo JSON\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(existing_data, f, indent=4)\n",
    "\n",
    "    # Liberar memoria\n",
    "    gc.collect()\n",
    "\n",
    "    # Finalmente, retornamos las emisiones de CO2\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para entrenar un modelo, rastrear las emisiones de CO2 y guardar la información de entrenamiento\n",
    "def track_training_C02_emissions(command, trained_model_folder, lossFunction, dataset):\n",
    "\n",
    "    # Inicializamos el tracker\n",
    "    tracker = EmissionsTracker()\n",
    "    \n",
    "    try:\n",
    "        # Obtenemos la fecha y hora de inicio\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        #iniciamos el tracker\n",
    "        tracker.start()\n",
    "        \n",
    "        # Ejecutamos el comando de entrenamiento\n",
    "        training_process = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "        # Detenemos el tracker y obtenemos las emisiones finales\n",
    "        emissions = tracker.stop()\n",
    "\n",
    "        # Obtenemos la fecha y hora de finalización\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        # Imprimimos la salida de la ejecución\n",
    "        print(f\"Salida de STDOUT: {training_process.stdout}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected Error: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Ruta del archivo JSON\n",
    "    json_file_path = os.path.join(\"..\", \"trained_models\", trained_model_folder ,\"trainingData.json\")\n",
    "    \n",
    "    # Leer el archivo JSON existente\n",
    "    existing_data = []\n",
    "    if os.path.exists(json_file_path):\n",
    "        try:\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                existing_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"El archivo {json_file_path} está vacío o contiene datos inválidos, se inicializará como una lista vacía.\")\n",
    "            existing_data = []\n",
    "\n",
    "    # Preparar la información del entrenamiento\n",
    "    training_info = {\n",
    "        \"training_iteration\": len(existing_data) + 1,  # Número de iteración basado en el tamaño del dataset existente\n",
    "        \"date\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"execution_time_seconds\": (end_time - start_time).total_seconds(),\n",
    "        \"CO2_emissions_kg\": emissions,\n",
    "        \"LossFunction\": lossFunction,\n",
    "        \"dataset\": dataset\n",
    "\n",
    "    }\n",
    "\n",
    "    # Agregar la nueva información del entrenamiento\n",
    "    existing_data.append(training_info)\n",
    "\n",
    "    # Escribir los datos actualizados al archivo JSON\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(existing_data, f, indent=4)\n",
    "\n",
    "    # Finalmente, retornamos las emisiones de CO2\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga los datasets, links disponibles en el README.\n",
    "dataset_path = \"../datasets/coveo_ecommerce\"\n",
    "model_path = \"../trained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Nos aseguramos de que tenemos funcionando la GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the preprocess script, specific to the dataset you chose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The preprocessing script in general, executes the following steps:\n",
    "    - Loads the raw data, with correct types\n",
    "    - Creates the sessions\n",
    "    - Removes duplicated items. An item is considered as a duplicate if the preceding (based on time) event in the same session contains the exact same item.\n",
    "    - Performes iterative support filtering\n",
    "        - Removes sessions with only one event\n",
    "        - Removes items with less than 5 events\n",
    "        - Until the size of the dataset changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2434084 1721538 234838\n",
      "973593 261047 124195\n",
      "828902 242962 39826\n",
      "794204 208264 39793\n",
      "784957 207891 37209\n",
      "781837 204771 37209\n",
      "780685 204724 36903\n",
      "780319 204358 36903\n",
      "780139 204353 36856\n",
      "780076 204290 36856\n",
      "780044 204288 36848\n",
      "780033 204277 36848\n",
      "780031 204277 36847\n",
      "780031 204277 36847\n",
      "780031 204277 36847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:143: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.min() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:146: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.max() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:143: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.min() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:146: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.max() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:143: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.min() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:146: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.max() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Dataset  NumEvents  NumSessions  \\\n",
      "0  retail_rocket\\retailrocket_processed_view_full...     780031       204277   \n",
      "1  retail_rocket\\retailrocket_processed_view_test...      29148         8036   \n",
      "2  retail_rocket\\retailrocket_processed_view_trai...     750832       196234   \n",
      "3  retail_rocket\\retailrocket_processed_view_trai...     716989       186821   \n",
      "4  retail_rocket\\retailrocket_processed_view_trai...      33812         9408   \n",
      "\n",
      "   NumItems     NumDays                   StartTime  \\\n",
      "0     36847  137.998819  2015-05-03 03:00:40.988000   \n",
      "1     11249    6.998421  2015-09-11 03:01:15.381000   \n",
      "2     36824  130.999083  2015-05-03 03:00:40.988000   \n",
      "3     36725  123.999511  2015-05-03 03:00:40.988000   \n",
      "4     12668    6.999157  2015-09-04 03:00:34.595000   \n",
      "\n",
      "                      EndTime  AvgItemViews  MinSessionLength  \\\n",
      "0  2015-09-18 02:58:58.914000     21.169457                 2   \n",
      "1  2015-09-18 02:58:58.914000      2.591164                 2   \n",
      "2  2015-09-11 02:59:21.734000     20.389746                 2   \n",
      "3  2015-09-04 02:59:58.751000     19.523186                 2   \n",
      "4  2015-09-11 02:59:21.734000      2.669087                 2   \n",
      "\n",
      "   MaxSessionLength  AvgSessionLength  MinSessionTime (sec)  \\\n",
      "0               260          3.818496                   0.0   \n",
      "1               200          3.627178                   0.0   \n",
      "2               260          3.826207                   0.0   \n",
      "3               260          3.837839                   0.0   \n",
      "4               147          3.593963                   0.0   \n",
      "\n",
      "   MaxSessionTime (sec)  \n",
      "0             46604.003  \n",
      "1             42992.512  \n",
      "2             46604.003  \n",
      "3             46604.003  \n",
      "4             43014.633  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:143: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.min() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:146: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.max() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:143: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.min() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "C:\\Users\\Juanc\\RecSys-Project\\RecSys-Project-2024-1\\Preprocess\\retailrocket_preproc.py:146: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt.datetime.utcfromtimestamp(data.Time.max() / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n"
     ]
    }
   ],
   "source": [
    "%run ../Preprocess/retailrocket_preproc.py --path $dataset_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a specific setup for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Especificar nombre del dataset para obtener sus mejores parámetros según experiment_setup.py, y los archivos de entrenamiento\n",
    "# y test en la carpeta datasets\n",
    "datasetParams = \"coveo\"\n",
    "datasetTrainFile =  \"coveo_processed_view_train_full.tsv\"\n",
    "datasetTestFile = \"coveo_processed_view_test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = setups[datasetParams][\"params_bprmax\"]\n",
    "params2 = setups[datasetParams][\"params_xe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file:  ../datasets/coveo_ecommerce\\coveo_processed_view_train_full.tsv\n",
      "Test file:  ../datasets/coveo_ecommerce\\coveo_processed_view_test.tsv\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(dataset_path,datasetTrainFile)\n",
    "test_path = os.path.join(dataset_path,datasetTestFile)\n",
    "\n",
    "print(\"Training file: \", train_path)\n",
    "print(\"Test file: \", test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_path, sep='\\t')\n",
    "num_sessions = df['SessionId'].nunique()\n",
    "num_items = df['ItemId'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de datos de cada columna:\n",
      "SessionId    int64\n",
      "ItemId       int64\n",
      "Time         int64\n",
      "dtype: object\n",
      "Number of distinct sessions: 165673\n",
      "Number of distinct items: 10868\n",
      "tamaño del dataset\n",
      "(1411113, 3)\n",
      "\n",
      "Primeras 10 filas del dataset:\n",
      "    SessionId  ItemId           Time\n",
      "0           2       1  1544909035879\n",
      "1           2       2  1544909231588\n",
      "2           2       3  1544909334734\n",
      "3           5       4  1545550811488\n",
      "4           5       5  1545550910607\n",
      "5          13      10  1545133945223\n",
      "6          13      11  1545133989522\n",
      "7          14      12  1544880554677\n",
      "8          14      13  1544880614907\n",
      "9          14      14  1544880676186\n",
      "10         14      15  1544880750349\n",
      "11         14      16  1544882106390\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los tipos de datos de cada columna\n",
    "print(\"Tipos de datos de cada columna:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"Number of distinct sessions: {num_sessions}\")\n",
    "\n",
    "print(f\"Number of distinct items: {num_items}\")\n",
    "\n",
    "\n",
    "print(\"tamaño del dataset\")\n",
    "print(df.shape)\n",
    "\n",
    "# Mostrar las primeras 10 filas del dataset\n",
    "print(\"\\nPrimeras 10 filas del dataset:\")\n",
    "print(df.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru4rec_pytorch_script(model_name, train_folder, train_data, test_data, model_path, loss, optim, final_act, layers, batch_size, dropout_p_embed, dropout_p_hidden, learning_rate, n_epochs, m, eval_hidden_reset, use_correct_loss, use_correct_mask_reset):\n",
    "    checkpoint_dir = f\"{model_path}\\\\{model_name}\"\n",
    "    s_train_full = (\n",
    "        f\"python ..\\\\GRU4REC-pytorch\\\\main.py --data_folder {train_folder} \"\n",
    "        f\"--train_data {train_data} --valid_data {test_data} --checkpoint_dir {checkpoint_dir} \"\n",
    "        f\"--num_layers 1 --embedding_dim {layers} --hidden_size {layers} \"\n",
    "        f\"--loss_type {'BPR-max' if loss == 'bpr-max' else 'CrossEntropy'} --final_act {final_act} \"\n",
    "        f\"--n_epochs {n_epochs} --batch_size {batch_size} --dropout_input {dropout_p_embed} \"\n",
    "        f\"--dropout_hidden {dropout_p_hidden} --lr {learning_rate} --momentum 0.0 \"\n",
    "        f\"--optimizer_type {'Adagrad' if optim == 'adagrad' else ''}\"\n",
    "        f\"{' --eval_hidden_reset' if eval_hidden_reset else ''}\"\n",
    "        f\"{' --use_correct_loss' if use_correct_loss else ''}\"\n",
    "        f\"{' --use_correct_mask_reset' if use_correct_mask_reset else ''}\"\n",
    "    )\n",
    "    s_test_full = s_train_full + f\" --is_eval --load_model {checkpoint_dir}\\\\model_0000{n_epochs-1}.pt --m {m}\"\n",
    "    return s_train_full, s_test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la configuración de los parámetros para el entrenamiento\n",
    "loss = params[\"loss\"]\n",
    "optim = params[\"optim\"]\n",
    "const_emb = params[\"constrained_embedding\"]\n",
    "embed = params[\"embedding\"]\n",
    "final_act = params[\"final_act\"]\n",
    "layers = params[\"layers\"]\n",
    "batch_size = params[\"batch_size\"]\n",
    "dropout_p_embed = params[\"dropout_p_embed\"]\n",
    "dropout_p_hidden = params[\"dropout_p_hidden\"]\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "momentum = params[\"momentum\"]\n",
    "sample_alpha = params[\"sample_alpha\"]\n",
    "bpreg = params[\"bpreg\"]\n",
    "logq = params[\"logq\"]\n",
    "hidden_act = params[\"hidden_act\"]\n",
    "n_epochs = 5\n",
    "m = '1 5 10 20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la configuración de los parámetros para el entrenamiento\n",
    "loss2 = params2[\"loss\"]\n",
    "optim2 = params2[\"optim\"]\n",
    "const_emb2 = params2[\"constrained_embedding\"]\n",
    "embed2 = params2[\"embedding\"]\n",
    "final_act2 = params2[\"final_act\"]\n",
    "layers2 = params2[\"layers\"]\n",
    "batch_size2 = params2[\"batch_size\"]\n",
    "dropout_p_embed2 = params2[\"dropout_p_embed\"]\n",
    "dropout_p_hidden2 = params2[\"dropout_p_hidden\"]\n",
    "learning_rate2 = params2[\"learning_rate\"]\n",
    "momentum2 = params2[\"momentum\"]\n",
    "sample_alpha2 = params2[\"sample_alpha\"]\n",
    "bpreg2 = params2[\"bpreg\"]\n",
    "logq2 = params2[\"logq\"]\n",
    "hidden_act2 = params2[\"hidden_act\"]\n",
    "n_epochs2 = 5\n",
    "m2 = '1 5 10 20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training folder:  ../datasets\n",
      "Train data:  coveo_ecommerce\\coveo_processed_view_train_full.tsv\n"
     ]
    }
   ],
   "source": [
    "train_folder, train_data = '/'.join(train_path.split('/')[:-1]), train_path.split('/')[-1]\n",
    "test_folder, test_data = '/'.join(test_path.split('/')[:-1]), test_path.split('/')[-1]\n",
    "\n",
    "print(\"Training folder: \", train_folder)\n",
    "print(\"Train data: \", train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & test the major fix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenar con BRP-MaxLoss\n",
    "train_script_majorfix_bpr, test_script_majorfix_bpr = create_gru4rec_pytorch_script(model_name='gru4rec_pytorch_majorfix_bprmax', train_folder=train_folder, train_data=train_data, test_data=test_data, model_path=model_path, loss=loss, optim=optim, final_act=final_act, layers=layers, batch_size=batch_size, dropout_p_embed=dropout_p_embed, dropout_p_hidden=dropout_p_hidden, learning_rate=learning_rate, n_epochs=n_epochs, m=m, eval_hidden_reset=True, use_correct_loss=True, use_correct_mask_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para entrenar con cros entropyLoss\n",
    "train_script_majorfix_xl, test_script_majorfix_xl = create_gru4rec_pytorch_script(model_name='gru4rec_pytorch_majorfix_bprmax', train_folder=train_folder, train_data=train_data, test_data=test_data, model_path=model_path, loss=loss2, optim=optim2, final_act=final_act2, layers=layers2, batch_size=batch_size2, dropout_p_embed=dropout_p_embed2, dropout_p_hidden=dropout_p_hidden2, learning_rate=learning_rate2, n_epochs=n_epochs2, m=m2, eval_hidden_reset=True, use_correct_loss=True, use_correct_mask_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ..\\GRU4REC-pytorch\\main.py --data_folder ../datasets --train_data coveo_ecommerce\\coveo_processed_view_train_full.tsv --valid_data coveo_ecommerce\\coveo_processed_view_test.tsv --checkpoint_dir ../trained_models\\gru4rec_pytorch_majorfix_bprmax --num_layers 1 --embedding_dim 512 --hidden_size 512 --loss_type BPR-max --final_act elu-1 --n_epochs 5 --batch_size 144 --dropout_input 0.35 --dropout_hidden 0.0 --lr 0.05 --momentum 0.0 --optimizer_type Adagrad --eval_hidden_reset --use_correct_loss --use_correct_mask_reset\n",
      "python ..\\GRU4REC-pytorch\\main.py --data_folder ../datasets --train_data coveo_ecommerce\\coveo_processed_view_train_full.tsv --valid_data coveo_ecommerce\\coveo_processed_view_test.tsv --checkpoint_dir ../trained_models\\gru4rec_pytorch_majorfix_bprmax --num_layers 1 --embedding_dim 512 --hidden_size 512 --loss_type BPR-max --final_act elu-1 --n_epochs 5 --batch_size 144 --dropout_input 0.35 --dropout_hidden 0.0 --lr 0.05 --momentum 0.0 --optimizer_type Adagrad --eval_hidden_reset --use_correct_loss --use_correct_mask_reset --is_eval --load_model ../trained_models\\gru4rec_pytorch_majorfix_bprmax\\model_00004.pt --m 1 5 10 20\n"
     ]
    }
   ],
   "source": [
    "#Imprimimos para verificar que esten bien los scripts\n",
    "print(train_script_majorfix_bpr)\n",
    "print(test_script_majorfix_bpr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the major fix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 22:06:02] Energy consumed for RAM : 0.010630 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:06:02] Energy consumed for all GPUs : 0.033113 kWh. Total GPU Power : 4.141227162465395 W\n",
      "[codecarbon INFO @ 22:06:02] Energy consumed for all CPUs : 0.040598 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:06:02] 0.084340 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:06:17] Energy consumed for RAM : 0.010676 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:06:17] Energy consumed for all GPUs : 0.033133 kWh. Total GPU Power : 4.907492951282068 W\n",
      "[codecarbon INFO @ 22:06:17] Energy consumed for all CPUs : 0.040775 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:06:17] 0.084585 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:06:32] Energy consumed for RAM : 0.010723 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:06:32] Energy consumed for all GPUs : 0.033141 kWh. Total GPU Power : 1.7722425445527203 W\n",
      "[codecarbon INFO @ 22:06:32] Energy consumed for all CPUs : 0.040952 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:06:32] 0.084815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:06:47] Energy consumed for RAM : 0.010769 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:06:47] Energy consumed for all GPUs : 0.033149 kWh. Total GPU Power : 2.011792026270382 W\n",
      "[codecarbon INFO @ 22:06:47] Energy consumed for all CPUs : 0.041129 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:06:47] 0.085047 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:07:02] Energy consumed for RAM : 0.010816 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:07:02] Energy consumed for all GPUs : 0.033159 kWh. Total GPU Power : 2.366666949267748 W\n",
      "[codecarbon INFO @ 22:07:02] Energy consumed for all CPUs : 0.041306 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:07:02] 0.085281 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:07:17] Energy consumed for RAM : 0.010862 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:07:17] Energy consumed for all GPUs : 0.033171 kWh. Total GPU Power : 2.8000276714267556 W\n",
      "[codecarbon INFO @ 22:07:17] Energy consumed for all CPUs : 0.041483 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:07:17] 0.085516 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:07:32] Energy consumed for RAM : 0.010908 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:07:32] Energy consumed for all GPUs : 0.033184 kWh. Total GPU Power : 3.221695297549958 W\n",
      "[codecarbon INFO @ 22:07:32] Energy consumed for all CPUs : 0.041661 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:07:32] 0.085753 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:07:47] Energy consumed for RAM : 0.010955 kWh. RAM Power : 11.1377534866333 W\n",
      "[codecarbon INFO @ 22:07:47] Energy consumed for all GPUs : 0.033200 kWh. Total GPU Power : 3.741711266515439 W\n",
      "[codecarbon INFO @ 22:07:47] Energy consumed for all CPUs : 0.041838 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:07:47] 0.085992 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "#elegimos el script con el que queremos trabajar\n",
    "for i in range(5):\n",
    "    emissions = track_training_C02_emissions(train_script_majorfix_bpr, \"gru4rec_pytorch_majorfix_bprmax\", \"BPR\", \"diginetica\")\n",
    "    if emissions is not None:\n",
    "        display(HTML(f\"<h2 style='color: green;'>Emisiones de CO2: {emissions} kg</h2>\"))\n",
    "    else:\n",
    "        display(HTML(\"<h2 style='color: red;'>Hubo un error durante la ejecución del comando.</h2>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the major fix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "                      Args                                             Values\n",
      "0              hidden_size                                                512\n",
      "1               num_layers                                                  1\n",
      "2               batch_size                                                144\n",
      "3            dropout_input                                               0.35\n",
      "4           dropout_hidden                                                0.0\n",
      "5                 n_epochs                                                  5\n",
      "6                        m                                     [1, 5, 10, 20]\n",
      "7           optimizer_type                                            Adagrad\n",
      "8                final_act                                              elu-1\n",
      "9                       lr                                               0.05\n",
      "10            weight_decay                                                  0\n",
      "11                momentum                                                0.0\n",
      "12                     eps                                           0.000001\n",
      "13                    seed                                                 22\n",
      "14                   sigma                                               None\n",
      "15           embedding_dim                                                512\n",
      "16               loss_type                                            BPR-max\n",
      "17               time_sort                                              False\n",
      "18              model_name                               GRU4REC-CrossEntropy\n",
      "19                save_dir                                             models\n",
      "20             data_folder                                        ../datasets\n",
      "21              train_data  coveo_ecommerce\\coveo_processed_view_train_ful...\n",
      "22              valid_data      coveo_ecommerce\\coveo_processed_view_test.tsv\n",
      "23                 is_eval                                               True\n",
      "24              load_model  ../trained_models\\gru4rec_pytorch_majorfix_bpr...\n",
      "25          checkpoint_dir  ../trained_models\\gru4rec_pytorch_majorfix_bprmax\n",
      "26       eval_hidden_reset                                               True\n",
      "27        use_correct_loss                                               True\n",
      "28  use_correct_mask_reset                                               True\n",
      "29                    cuda                                               True\n",
      "Loading train data from ../datasets\\coveo_ecommerce\\coveo_processed_view_train_full.tsv\n",
      "Loading valid data from ../datasets\\coveo_ecommerce\\coveo_processed_view_test.tsv\n",
      "Loading pre-trained model from ../trained_models\\gru4rec_pytorch_majorfix_bprmax\\model_00004.pt\n",
      "Effectivetly Loading pre-trained model from ../trained_models\\gru4rec_pytorch_majorfix_bprmax\\model_00004.pt\n",
      "Recall@1: 0.01806478 MRR@1: 0.01806478\n",
      "Recall@5: 0.06030823 MRR@5: 0.03229051\n",
      "Recall@10: 0.09936785 MRR@10: 0.03743755\n",
      "Recall@20: 0.15457733 MRR@20: 0.04121002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "# Elegimos el script de test con el que queremos trabajar\n",
    "try:\n",
    "    # Ejecutar el comando\n",
    "    result = subprocess.run(test_script_majorfix_bpr, shell=True, capture_output=True, text=True)\n",
    "    # Imprimir la salida estándar\n",
    "    print(result.stdout)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
